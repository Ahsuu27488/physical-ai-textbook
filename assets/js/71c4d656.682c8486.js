"use strict";(globalThis.webpackChunkmy_website_1=globalThis.webpackChunkmy_website_1||[]).push([[7573],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}},8590:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-3/lesson-2-synthetic-data-generation","title":"Lesson 2 - Synthetic Data Generation","description":"Learning Objectives","source":"@site/docs/module-3/lesson-2-synthetic-data-generation.md","sourceDirName":"module-3","slug":"/module-3/lesson-2-synthetic-data-generation","permalink":"/physical-ai-textbook/docs/module-3/lesson-2-synthetic-data-generation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3/lesson-2-synthetic-data-generation.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Lesson 2 - Synthetic Data Generation","sidebar_position":5},"sidebar":"textbookSidebar","previous":{"title":"Lesson 1 - Isaac Sim Basics and Setup","permalink":"/physical-ai-textbook/docs/module-3/lesson-1-isaac-sim-basics"},"next":{"title":"Lesson 3 - Isaac ROS Integration","permalink":"/physical-ai-textbook/docs/module-3/lesson-3-isaac-ros-integration"}}');var i=t(4848),o=t(8453);const s={title:"Lesson 2 - Synthetic Data Generation",sidebar_position:5},r="Lesson 2: Synthetic Data Generation",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Synthetic Data",id:"introduction-to-synthetic-data",level:2},{value:"Why Synthetic Data Matters",id:"why-synthetic-data-matters",level:3},{value:"Isaac Sim&#39;s Synthetic Data Tools",id:"isaac-sims-synthetic-data-tools",level:2},{value:"Isaac Sim Sensors Extension",id:"isaac-sim-sensors-extension",level:3},{value:"RGB and Depth Data Generation",id:"rgb-and-depth-data-generation",level:3},{value:"Domain Randomization",id:"domain-randomization",level:2},{value:"Lighting Randomization",id:"lighting-randomization",level:3},{value:"Texture and Material Randomization",id:"texture-and-material-randomization",level:3},{value:"Creating a Synthetic Dataset Pipeline",id:"creating-a-synthetic-dataset-pipeline",level:2},{value:"Basic Data Collection Script",id:"basic-data-collection-script",level:3},{value:"Semantic Segmentation Data",id:"semantic-segmentation-data",level:2},{value:"Instance Segmentation",id:"instance-segmentation",level:2},{value:"Depth and Normal Data",id:"depth-and-normal-data",level:2},{value:"Object Detection Annotations",id:"object-detection-annotations",level:2},{value:"Practical Exercise: Create a Custom Dataset",id:"practical-exercise-create-a-custom-dataset",level:2},{value:"Implementation Steps:",id:"implementation-steps",level:3},{value:"COCO Format Annotations",id:"coco-format-annotations",level:2},{value:"Quality Assurance",id:"quality-assurance",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lesson-2-synthetic-data-generation",children:"Lesson 2: Synthetic Data Generation"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this lesson, students will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Generate synthetic datasets for computer vision tasks"}),"\n",(0,i.jsx)(n.li,{children:"Create labeled training data in Isaac Sim"}),"\n",(0,i.jsx)(n.li,{children:"Understand domain randomization techniques"}),"\n",(0,i.jsx)(n.li,{children:"Apply synthetic-to-real transfer methods"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-synthetic-data",children:"Introduction to Synthetic Data"}),"\n",(0,i.jsx)(n.p,{children:"Synthetic data generation is a critical component of modern AI development, especially for robotics applications. Isaac Sim enables the creation of large, diverse, and perfectly labeled datasets that would be expensive or impossible to collect in the real world."}),"\n",(0,i.jsx)(n.h3,{id:"why-synthetic-data-matters",children:"Why Synthetic Data Matters"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost-Effective"}),": Generate thousands of images without physical setup"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perfect Labels"}),": Automatic ground truth for segmentation, detection, etc."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety"}),": Test dangerous scenarios without risk"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Variety"}),": Control lighting, textures, backgrounds, and object poses"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scalability"}),": Generate data 24/7 without human intervention"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"isaac-sims-synthetic-data-tools",children:"Isaac Sim's Synthetic Data Tools"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-sim-sensors-extension",children:"Isaac Sim Sensors Extension"}),"\n",(0,i.jsx)(n.p,{children:"Isaac Sim provides comprehensive sensor simulation with realistic noise models:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from omni.isaac.core.utils.prims import create_prim\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\n# Create a camera prim\ncamera_prim_path = "/World/Robot/Camera"\ncreate_prim(camera_prim_path, "Camera", position=np.array([0.0, 0.0, 0.0]))\n\n# Add camera to the scene\ncamera = Camera(\n    prim_path=camera_prim_path,\n    frequency=30,  # Hz\n    resolution=(640, 480)\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"rgb-and-depth-data-generation",children:"RGB and Depth Data Generation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from omni.isaac.synthetic_utils import plot\nimport carb\n\n# Enable RGB and depth sensors\ncamera.add_color_render_product("/World/Robot/Camera")\ncamera.add_depth_to_world_render_product("/World/Robot/Camera")\n\n# Capture data\nrgb_data = camera.get_rgb()\ndepth_data = camera.get_depth()\n\n# Save data\ncarb.log_info(f"RGB shape: {rgb_data.shape}")\ncarb.log_info(f"Depth shape: {depth_data.shape}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,i.jsx)(n.p,{children:"Domain randomization is a technique that varies environmental parameters to make models more robust:"}),"\n",(0,i.jsx)(n.h3,{id:"lighting-randomization",children:"Lighting Randomization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import random\nfrom pxr import Gf\n\ndef randomize_lighting(world):\n    """Randomize lighting conditions in the scene"""\n    # Get all lights in the scene\n    lights = world.scene.get_all_lights()\n\n    for light in lights:\n        # Randomize intensity\n        intensity = random.uniform(50, 1000)\n        light.set_intensity(intensity)\n\n        # Randomize color temperature\n        color_temp = random.uniform(3000, 8000)\n        light.set_color_temperature(color_temp)\n\n        # Randomize position slightly\n        current_pos = light.get_world_pose()[0]\n        new_pos = current_pos + np.array([\n            random.uniform(-0.5, 0.5),\n            random.uniform(-0.5, 0.5),\n            random.uniform(-0.5, 0.5)\n        ])\n        light.set_world_pose(position=new_pos)\n\ndef randomize_materials(world):\n    """Randomize material properties"""\n    # Randomize object colors, textures, and reflectance\n    objects = world.scene.get_all_objects()\n\n    for obj in objects:\n        # Randomize diffuse color\n        diffuse_color = (random.uniform(0, 1),\n                        random.uniform(0, 1),\n                        random.uniform(0, 1))\n        # Apply to material\n'})}),"\n",(0,i.jsx)(n.h3,{id:"texture-and-material-randomization",children:"Texture and Material Randomization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def randomize_textures(world):\n    """Apply random textures to objects"""\n    import omni.kit.asset_editor as asset_editor\n\n    # Define texture library\n    texture_library = [\n        "/Isaac/Textures/Metal/metal_01.png",\n        "/Isaac/Textures/Wood/wood_01.png",\n        "/Isaac/Textures/Plastic/plastic_01.png",\n        # Add more textures\n    ]\n\n    objects = world.scene.get_all_objects()\n\n    for obj in objects:\n        # Randomly assign texture\n        random_texture = random.choice(texture_library)\n        # Apply texture to object\'s material\n'})}),"\n",(0,i.jsx)(n.h2,{id:"creating-a-synthetic-dataset-pipeline",children:"Creating a Synthetic Dataset Pipeline"}),"\n",(0,i.jsx)(n.h3,{id:"basic-data-collection-script",children:"Basic Data Collection Script"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import os\nimport json\nimport numpy as np\nfrom PIL import Image\nfrom omni.isaac.core import World\nfrom omni.isaac.sensor import Camera\nimport carb\n\nclass SyntheticDataGenerator:\n    def __init__(self, world, camera, output_dir="synthetic_data"):\n        self.world = world\n        self.camera = camera\n        self.output_dir = output_dir\n        self.data_counter = 0\n\n        # Create output directories\n        os.makedirs(f"{output_dir}/rgb", exist_ok=True)\n        os.makedirs(f"{output_dir}/depth", exist_ok=True)\n        os.makedirs(f"{output_dir}/labels", exist_ok=True)\n\n        # Data annotations\n        self.annotations = []\n\n    def capture_frame(self):\n        """Capture a single frame with all modalities"""\n        # Get sensor data\n        rgb_image = self.camera.get_rgb()\n        depth_image = self.camera.get_depth()\n        segmentation = self.camera.get_semantic_segmentation()\n\n        # Generate filename\n        filename = f"frame_{self.data_counter:06d}"\n\n        # Save RGB image\n        rgb_pil = Image.fromarray(rgb_image)\n        rgb_pil.save(f"{self.output_dir}/rgb/{filename}.png")\n\n        # Save depth image\n        depth_pil = Image.fromarray(depth_image)\n        depth_pil.save(f"{self.output_dir}/depth/{filename}.png")\n\n        # Save segmentation\n        seg_pil = Image.fromarray(segmentation)\n        seg_pil.save(f"{self.output_dir}/labels/{filename}.png")\n\n        # Create annotation\n        annotation = {\n            "filename": filename,\n            "rgb_path": f"rgb/{filename}.png",\n            "depth_path": f"depth/{filename}.png",\n            "labels_path": f"labels/{filename}.png",\n            "timestamp": self.world.current_time_step_index,\n            "camera_pose": self.camera.get_world_pose(),\n            "objects_in_scene": self.get_scene_objects()\n        }\n\n        self.annotations.append(annotation)\n        self.data_counter += 1\n\n        carb.log_info(f"Captured frame {filename}")\n\n    def get_scene_objects(self):\n        """Get information about objects in the scene"""\n        objects = []\n        for prim in self.world.scene.get_objects():\n            pose = prim.get_world_pose()\n            size = prim.get_world_size()\n            objects.append({\n                "name": prim.name,\n                "position": pose[0].tolist(),\n                "orientation": pose[1].tolist(),\n                "size": size\n            })\n        return objects\n\n    def save_annotations(self):\n        """Save all annotations to JSON file"""\n        with open(f"{self.output_dir}/annotations.json", "w") as f:\n            json.dump(self.annotations, f, indent=2)\n\n# Usage example\ndef main():\n    world = World(stage_units_in_meters=1.0)\n\n    # Add camera to scene\n    camera = Camera(\n        prim_path="/World/Robot/Camera",\n        frequency=30,\n        resolution=(640, 480)\n    )\n\n    # Create data generator\n    generator = SyntheticDataGenerator(world, camera)\n\n    # Collect data\n    for i in range(1000):  # Collect 1000 frames\n        # Randomize scene\n        randomize_lighting(world)\n        randomize_textures(world)\n\n        # Move objects randomly\n        randomize_object_poses(world)\n\n        # Capture frame\n        generator.capture_frame()\n\n        # Step simulation\n        world.step(render=True)\n\n    # Save annotations\n    generator.save_annotations()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"semantic-segmentation-data",children:"Semantic Segmentation Data"}),"\n",(0,i.jsx)(n.p,{children:"Semantic segmentation requires pixel-level labels for each object class:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def generate_segmentation_labels(world, camera):\n    """Generate semantic segmentation labels"""\n    # Enable semantic segmentation\n    camera.add_semantic_segmentation_render_product("/World/Robot/Camera")\n\n    # Get segmentation data\n    seg_data = camera.get_semantic_segmentation()\n\n    # Create class mapping\n    class_mapping = {\n        "robot": 1,\n        "object": 2,\n        "background": 0,\n        "floor": 3,\n        # Add more classes as needed\n    }\n\n    # Apply mapping to create labeled image\n    labeled_image = np.zeros(seg_data.shape, dtype=np.uint8)\n    for class_name, class_id in class_mapping.items():\n        mask = (seg_data == class_name)\n        labeled_image[mask] = class_id\n\n    return labeled_image\n'})}),"\n",(0,i.jsx)(n.h2,{id:"instance-segmentation",children:"Instance Segmentation"}),"\n",(0,i.jsx)(n.p,{children:"Instance segmentation distinguishes between different instances of the same class:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def generate_instance_labels(world, camera):\n    """Generate instance segmentation labels"""\n    # Get instance segmentation data\n    instance_data = camera.get_instance_segmentation()\n\n    # Create unique IDs for each instance\n    unique_instances = np.unique(instance_data)\n\n    instance_labels = np.zeros(instance_data.shape, dtype=np.uint16)\n    for i, instance_id in enumerate(unique_instances):\n        mask = (instance_data == instance_id)\n        instance_labels[mask] = i + 1  # Start from 1, 0 is background\n\n    return instance_labels\n'})}),"\n",(0,i.jsx)(n.h2,{id:"depth-and-normal-data",children:"Depth and Normal Data"}),"\n",(0,i.jsx)(n.p,{children:"Depth and surface normal data are crucial for 3D understanding:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def capture_3d_data(camera):\n    """Capture depth and normal data"""\n    # Depth data\n    depth_data = camera.get_depth()\n\n    # Surface normals\n    normal_data = camera.get_normals()\n\n    # Convert to formats suitable for training\n    depth_normalized = (depth_data - depth_data.min()) / (depth_data.max() - depth_data.min())\n    normal_normalized = (normal_data + 1) / 2  # Normalize from [-1,1] to [0,1]\n\n    return depth_normalized, normal_normalized\n'})}),"\n",(0,i.jsx)(n.h2,{id:"object-detection-annotations",children:"Object Detection Annotations"}),"\n",(0,i.jsx)(n.p,{children:"For object detection tasks, we need bounding box annotations:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def generate_bounding_boxes(world, camera):\n    """Generate 2D bounding box annotations"""\n    annotations = []\n\n    # Get all objects in the scene\n    objects = world.scene.get_all_objects()\n\n    for obj in objects:\n        # Get 3D bounding box in world coordinates\n        bbox_3d = obj.get_world_bounding_box()\n\n        # Project to 2D camera coordinates\n        bbox_2d = project_3d_bbox_to_2d(bbox_3d, camera.get_intrinsics())\n\n        # Create annotation\n        annotation = {\n            "object_name": obj.name,\n            "bbox_2d": bbox_2d.tolist(),\n            "bbox_3d": {\n                "center": bbox_3d.center.tolist(),\n                "size": bbox_3d.size.tolist()\n            },\n            "visibility": calculate_visibility(bbox_2d, camera.resolution)\n        }\n\n        annotations.append(annotation)\n\n    return annotations\n\ndef project_3d_bbox_to_2d(bbox_3d, camera_intrinsics):\n    """Project 3D bounding box to 2D image coordinates"""\n    # Implementation of 3D to 2D projection\n    # This involves using camera intrinsic matrix\n    # and projecting 3D points to 2D image plane\n    pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"practical-exercise-create-a-custom-dataset",children:"Practical Exercise: Create a Custom Dataset"}),"\n",(0,i.jsx)(n.p,{children:"Create a synthetic dataset for object detection with the following requirements:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scene Setup"}),": Create a scene with multiple objects of different classes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Randomization"}),": Implement domain randomization for lighting and textures"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Capture"}),": Capture RGB, depth, and bounding box annotations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Variety"}),": Generate 1000 diverse frames with objects in different positions"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Setup the scene"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Create a scene with various objects\nobjects = [\n    {"name": "cube", "type": "cube", "color": "red"},\n    {"name": "sphere", "type": "sphere", "color": "blue"},\n    {"name": "cylinder", "type": "cylinder", "color": "green"}\n]\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Implement randomization"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Random object positions"}),"\n",(0,i.jsx)(n.li,{children:"Random lighting conditions"}),"\n",(0,i.jsx)(n.li,{children:"Random backgrounds"}),"\n",(0,i.jsx)(n.li,{children:"Random camera viewpoints"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Capture and save data"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"RGB images"}),"\n",(0,i.jsx)(n.li,{children:"Depth maps"}),"\n",(0,i.jsx)(n.li,{children:"Annotations in COCO format"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"coco-format-annotations",children:"COCO Format Annotations"}),"\n",(0,i.jsx)(n.p,{children:"For compatibility with popular computer vision frameworks:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def create_coco_annotations(frames_data, categories):\n    """Create COCO format annotations"""\n    coco_format = {\n        "info": {\n            "description": "Synthetic Dataset",\n            "version": "1.0",\n            "year": 2024\n        },\n        "licenses": [{"id": 1, "name": "Synthetic Data License"}],\n        "categories": categories,\n        "images": [],\n        "annotations": []\n    }\n\n    annotation_id = 1\n    for frame_idx, frame in enumerate(frames_data):\n        # Add image info\n        image_info = {\n            "id": frame_idx,\n            "file_name": frame["filename"],\n            "width": 640,\n            "height": 480,\n            "date_captured": "2024"\n        }\n        coco_format["images"].append(image_info)\n\n        # Add annotations\n        for obj in frame["objects"]:\n            bbox = obj["bbox_2d"]\n            annotation = {\n                "id": annotation_id,\n                "image_id": frame_idx,\n                "category_id": obj["category_id"],\n                "bbox": [bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1]],  # x, y, width, height\n                "area": (bbox[2]-bbox[0]) * (bbox[3]-bbox[1]),\n                "iscrowd": 0\n            }\n            coco_format["annotations"].append(annotation)\n            annotation_id += 1\n\n    return coco_format\n'})}),"\n",(0,i.jsx)(n.h2,{id:"quality-assurance",children:"Quality Assurance"}),"\n",(0,i.jsx)(n.p,{children:"Ensure synthetic data quality:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Visual Inspection"}),": Manually check a sample of generated images"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Statistical Analysis"}),": Verify distribution of objects, lighting, etc."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Consistency Checks"}),": Ensure annotations match the visual content"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Realism Verification"}),": Compare synthetic and real data distributions"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This lesson covered synthetic data generation techniques in Isaac Sim, including RGB, depth, segmentation, and detection datasets. Synthetic data is crucial for training robust AI models in robotics applications."}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"In the next lesson, we'll explore Isaac ROS packages and how to bridge synthetic data with real-world robotics applications."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);