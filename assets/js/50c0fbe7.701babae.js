"use strict";(globalThis.webpackChunkmy_website_1=globalThis.webpackChunkmy_website_1||[]).push([[1921],{1347:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/lesson-3-vla-integration","title":"Lesson 3 - VLA Integration and Autonomous Systems","description":"Learning Objectives","source":"@site/docs/module-4/lesson-3-vla-integration.md","sourceDirName":"module-4","slug":"/module-4/lesson-3-vla-integration","permalink":"/physical-ai-textbook/docs/module-4/lesson-3-vla-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/lesson-3-vla-integration.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Lesson 3 - VLA Integration and Autonomous Systems","sidebar_position":6},"sidebar":"textbookSidebar","previous":{"title":"Lesson 2 - Cognitive Planning and Action Generation","permalink":"/physical-ai-textbook/docs/module-4/lesson-2-cognitive-planning"},"next":{"title":"Weeks 1-2 - Introduction to Physical AI","permalink":"/physical-ai-textbook/docs/week-01-02/intro"}}');var o=s(4848),a=s(8453);const r={title:"Lesson 3 - VLA Integration and Autonomous Systems",sidebar_position:6},i="Lesson 3: VLA Integration and Autonomous Systems",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to VLA Integration",id:"introduction-to-vla-integration",level:2},{value:"VLA System Architecture",id:"vla-system-architecture",level:3},{value:"Complete VLA System Implementation",id:"complete-vla-system-implementation",level:2},{value:"Voice Command Processing Node",id:"voice-command-processing-node",level:3},{value:"Conversational Interface for Robots",id:"conversational-interface-for-robots",level:2},{value:"Dialogue Manager Node",id:"dialogue-manager-node",level:3},{value:"Autonomous Humanoid System",id:"autonomous-humanoid-system",level:2},{value:"Complete Autonomous System Node",id:"complete-autonomous-system-node",level:3},{value:"Launch File for Complete VLA System",id:"launch-file-for-complete-vla-system",level:2},{value:"Complete VLA System Launch",id:"complete-vla-system-launch",level:3},{value:"Capstone Project: Autonomous Humanoid Demonstration",id:"capstone-project-autonomous-humanoid-demonstration",level:2},{value:"Complete System Integration Example",id:"complete-system-integration-example",level:3},{value:"Practical Exercise: Complete VLA System",id:"practical-exercise-complete-vla-system",level:2},{value:"Implementation Steps:",id:"implementation-steps",level:3},{value:"Troubleshooting VLA Systems",id:"troubleshooting-vla-systems",level:2},{value:"1. Integration Issues",id:"1-integration-issues",level:3},{value:"2. Performance Issues",id:"2-performance-issues",level:3},{value:"3. Safety Concerns",id:"3-safety-concerns",level:3},{value:"4. Recognition Errors",id:"4-recognition-errors",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"lesson-3-vla-integration-and-autonomous-systems",children:"Lesson 3: VLA Integration and Autonomous Systems"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this lesson, students will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate Vision-Language-Action systems for autonomous robot control"}),"\n",(0,o.jsx)(n.li,{children:"Implement end-to-end pipelines from voice commands to robot actions"}),"\n",(0,o.jsx)(n.li,{children:"Create conversational interfaces for robot interaction"}),"\n",(0,o.jsx)(n.li,{children:"Build autonomous humanoid demonstration systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-vla-integration",children:"Introduction to VLA Integration"}),"\n",(0,o.jsx)(n.p,{children:"Vision-Language-Action (VLA) integration represents the convergence of three critical AI technologies that enable truly autonomous robots. This integration allows robots to understand natural language commands, perceive their environment visually, and execute complex actions in response."}),"\n",(0,o.jsx)(n.h3,{id:"vla-system-architecture",children:"VLA System Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Voice Command \u2192 NLP \u2192 Task Planning \u2192 Action Execution \u2192 Environment \u2192 Perception \u2192 Feedback\n     \u2191              \u2193                    \u2193                    \u2193              \u2191         \u2191\n   (Human)     (Understanding)    (Planning)          (Execution)    (Sensors) (Feedback)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"complete-vla-system-implementation",children:"Complete VLA System Implementation"}),"\n",(0,o.jsx)(n.h3,{id:"voice-command-processing-node",children:"Voice Command Processing Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nfrom vision_language_interfaces.msg import VisionLanguageCommand, VisionLanguageResponse\nimport speech_recognition as sr\nimport openai\nimport asyncio\nimport threading\nfrom queue import Queue\nfrom cv_bridge import CvBridge\n\nclass VLACommandProcessor(Node):\n    def __init__(self):\n        super().__init__(\'vla_command_processor\')\n\n        # Initialize components\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.bridge = CvBridge()\n\n        # Configuration\n        self.llm_api_key = "your-openai-api-key"  # In practice, use secure storage\n        openai.api_key = self.llm_api_key\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.command_sub = self.create_subscription(\n            String, \'/vla/voice_command\', self.command_callback, 10)\n\n        # Create publishers\n        self.response_pub = self.create_publisher(\n            VisionLanguageResponse, \'/vla/response\', 10)\n        self.action_pub = self.create_publisher(\n            String, \'/robot/action_command\', 10)\n\n        # Initialize state\n        self.current_image = None\n        self.command_queue = Queue()\n        self.processing_lock = threading.Lock()\n\n        # Start voice recognition thread\n        self.voice_thread = threading.Thread(target=self.voice_recognition_loop, daemon=True)\n        self.voice_thread.start()\n\n        self.get_logger().info(\'VLA Command Processor initialized\')\n\n    def image_callback(self, msg):\n        """Handle incoming camera images"""\n        try:\n            self.current_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n            self.get_logger().debug(\'Image updated\')\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def command_callback(self, msg):\n        """Handle text commands"""\n        self.get_logger().info(f\'Received command: {msg.data}\')\n        self.process_command(msg.data)\n\n    def voice_recognition_loop(self):\n        """Continuous voice recognition in separate thread"""\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        while rclpy.ok():\n            try:\n                with self.microphone as source:\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\n\n                # Recognize speech\n                command = self.recognizer.recognize_google(audio)\n                self.get_logger().info(f\'Recognized: {command}\')\n\n                # Publish recognized command\n                cmd_msg = String()\n                cmd_msg.data = command\n                self.command_callback(cmd_msg)\n\n            except sr.WaitTimeoutError:\n                # No speech detected, continue loop\n                continue\n            except sr.UnknownValueError:\n                self.get_logger().warn(\'Could not understand audio\')\n            except sr.RequestError as e:\n                self.get_logger().error(f\'Speech recognition error: {e}\')\n            except Exception as e:\n                self.get_logger().error(f\'Voice recognition error: {e}\')\n\n    def process_command(self, command):\n        """Process a natural language command"""\n        with self.processing_lock:\n            try:\n                # Step 1: Parse command using LLM\n                parsed_command = self.parse_command_with_llm(command)\n\n                # Step 2: Get visual context\n                visual_context = self.get_visual_context()\n\n                # Step 3: Generate action plan\n                action_plan = self.generate_action_plan(parsed_command, visual_context)\n\n                # Step 4: Execute action plan\n                result = self.execute_action_plan(action_plan)\n\n                # Step 5: Generate response\n                response = self.generate_response(command, action_plan, result)\n\n                # Publish response\n                response_msg = VisionLanguageResponse()\n                response_msg.command = command\n                response_msg.response = response\n                response_msg.success = result[\'success\']\n                response_msg.timestamp = self.get_clock().now().to_msg()\n\n                self.response_pub.publish(response_msg)\n\n            except Exception as e:\n                self.get_logger().error(f\'Error processing command: {e}\')\n                error_response = VisionLanguageResponse()\n                error_response.command = command\n                error_response.response = f"Error processing command: {str(e)}"\n                error_response.success = False\n                self.response_pub.publish(error_response)\n\n    def parse_command_with_llm(self, command):\n        """Parse command using LLM"""\n        prompt = f"""\n        Parse the following natural language command into structured action:\n        Command: "{command}"\n\n        Return a JSON object with:\n        - action: primary action (e.g., "navigate", "grasp", "clean", "inspect")\n        - target_object: target object if applicable\n        - target_location: target location if applicable\n        - additional_parameters: any additional parameters\n\n        Example response:\n        {{\n            "action": "grasp",\n            "target_object": "red cup",\n            "target_location": null,\n            "additional_parameters": {{"height": "table_level"}}\n        }}\n        """\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.1,\n                max_tokens=200\n            )\n\n            import json\n            result = json.loads(response.choices[0].message[\'content\'])\n            return result\n        except Exception as e:\n            self.get_logger().error(f\'LLM parsing error: {e}\')\n            # Fallback to simple parsing\n            return self.fallback_parse_command(command)\n\n    def fallback_parse_command(self, command):\n        """Simple command parsing as fallback"""\n        command_lower = command.lower()\n\n        if any(word in command_lower for word in [\'go to\', \'navigate\', \'move to\']):\n            return {"action": "navigate", "target_object": None, "target_location": "specified location", "additional_parameters": {}}\n        elif any(word in command_lower for word in [\'pick up\', \'grasp\', \'get\', \'take\']):\n            return {"action": "grasp", "target_object": "object", "target_location": None, "additional_parameters": {}}\n        elif any(word in command_lower for word in [\'clean\', \'wipe\', \'dust\']):\n            return {"action": "clean", "target_object": "surface", "target_location": None, "additional_parameters": {}}\n        else:\n            return {"action": "unknown", "target_object": None, "target_location": None, "additional_parameters": {}}\n\n    def get_visual_context(self):\n        """Get visual context from current image"""\n        if self.current_image is None:\n            return {"objects": [], "locations": [], "description": "No visual data available"}\n\n        # In real implementation, this would use object detection, segmentation, etc.\n        # For demonstration, we\'ll return mock data\n        return {\n            "objects": ["red cup", "blue book", "white table", "black chair"],\n            "locations": ["kitchen", "living room", "bedroom"],\n            "description": "A room with furniture and objects"\n        }\n\n    def generate_action_plan(self, parsed_command, visual_context):\n        """Generate detailed action plan based on parsed command and visual context"""\n        action = parsed_command[\'action\']\n        target_object = parsed_command[\'target_object\']\n        target_location = parsed_command[\'target_location\']\n\n        plan = []\n\n        if action == \'grasp\' and target_object:\n            # Find object in visual context\n            if target_object in visual_context[\'objects\']:\n                plan = [\n                    {"action": "find_object", "parameters": {"object_name": target_object}},\n                    {"action": "navigate_to_object", "parameters": {"object_name": target_object}},\n                    {"action": "grasp_object", "parameters": {"object_name": target_object}},\n                    {"action": "verify_grasp", "parameters": {"object_name": target_object}}\n                ]\n            else:\n                # Object not found, search for it\n                plan = [\n                    {"action": "search_for_object", "parameters": {"object_name": target_object}},\n                    {"action": "grasp_object", "parameters": {"object_name": target_object}},\n                    {"action": "verify_grasp", "parameters": {"object_name": target_object}}\n                ]\n\n        elif action == \'navigate\' and target_location:\n            plan = [\n                {"action": "plan_path", "parameters": {"destination": target_location}},\n                {"action": "execute_navigation", "parameters": {"destination": target_location}},\n                {"action": "verify_arrival", "parameters": {"destination": target_location}}\n            ]\n\n        elif action == \'clean\':\n            plan = [\n                {"action": "find_surface", "parameters": {"surface_type": "cleanable"}},\n                {"action": "navigate_to_surface", "parameters": {"surface_type": "cleanable"}},\n                {"action": "clean_surface", "parameters": {"surface_type": "cleanable"}},\n                {"action": "verify_cleaning", "parameters": {"surface_type": "cleanable"}}\n            ]\n\n        else:\n            plan = [{"action": "unknown_command", "parameters": {"command": parsed_command}}]\n\n        return plan\n\n    async def execute_action_plan_async(self, plan):\n        """Execute action plan asynchronously"""\n        results = []\n\n        for step in plan:\n            self.get_logger().info(f\'Executing: {step["action"]}\')\n\n            # In real implementation, this would call actual robot actions\n            # For demonstration, we\'ll simulate execution\n            result = await self.execute_single_action(step)\n            results.append(result)\n\n            if not result[\'success\']:\n                self.get_logger().error(f\'Action failed: {step["action"]}\')\n                break\n\n        return {"success": all(r[\'success\'] for r in results), "results": results}\n\n    def execute_single_action(self, action_step):\n        """Execute a single action step"""\n        # Simulate action execution\n        import asyncio\n        asyncio.sleep(0.1)  # Simulate execution time\n\n        # In real implementation, this would interface with robot controllers\n        # For now, we\'ll return success for demonstration\n        return {\n            "action": action_step["action"],\n            "success": True,\n            "details": f"Executed {action_step[\'action\']} with parameters {action_step[\'parameters\']}"\n        }\n\n    def execute_action_plan(self, plan):\n        """Execute action plan (synchronous wrapper)"""\n        # In a real implementation, this would use asyncio properly\n        # For demonstration, we\'ll use a simplified approach\n        results = []\n\n        for step in plan:\n            self.get_logger().info(f\'Executing: {step["action"]}\')\n\n            # Simulate execution\n            result = self.execute_single_action(step)\n            results.append(result)\n\n            if not result[\'success\']:\n                break\n\n        return {"success": all(r[\'success\'] for r in results), "results": results}\n\n    def generate_response(self, original_command, action_plan, execution_result):\n        """Generate natural language response"""\n        if execution_result[\'success\']:\n            return f"I have completed the task: {original_command}. All actions were successful."\n        else:\n            return f"I encountered an issue while executing: {original_command}. Some actions failed."\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = VLACommandProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"conversational-interface-for-robots",children:"Conversational Interface for Robots"}),"\n",(0,o.jsx)(n.h3,{id:"dialogue-manager-node",children:"Dialogue Manager Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom vision_language_interfaces.msg import VisionLanguageResponse\nimport openai\nimport json\nfrom collections import deque\n\nclass RobotDialogueManager(Node):\n    def __init__(self):\n        super().__init__(\'robot_dialogue_manager\')\n\n        # Initialize LLM\n        self.llm_api_key = "your-openai-api-key"\n        openai.api_key = self.llm_api_key\n\n        # Create subscribers\n        self.user_input_sub = self.create_subscription(\n            String, \'/user_input\', self.user_input_callback, 10)\n        self.vla_response_sub = self.create_subscription(\n            VisionLanguageResponse, \'/vla/response\', self.vla_response_callback, 10)\n\n        # Create publishers\n        self.robot_speech_pub = self.create_publisher(\n            String, \'/robot/speech_output\', 10)\n        self.vla_command_pub = self.create_publisher(\n            String, \'/vla/voice_command\', 10)\n\n        # Initialize conversation state\n        self.conversation_history = deque(maxlen=10)  # Keep last 10 exchanges\n        self.waiting_for_vla_response = False\n        self.pending_user_query = None\n\n        self.get_logger().info(\'Robot Dialogue Manager initialized\')\n\n    def user_input_callback(self, msg):\n        """Handle user input"""\n        user_text = msg.data\n        self.get_logger().info(f\'User said: {user_text}\')\n\n        # Add to conversation history\n        self.conversation_history.append({"role": "user", "content": user_text})\n\n        # Process the input\n        response = self.process_user_input(user_text)\n\n        # Publish response\n        response_msg = String()\n        response_msg.data = response\n        self.robot_speech_pub.publish(response_msg)\n\n        # Add to conversation history\n        self.conversation_history.append({"role": "assistant", "content": response})\n\n    def vla_response_callback(self, msg):\n        """Handle VLA system response"""\n        if self.waiting_for_vla_response and self.pending_user_query:\n            # Generate a conversational response based on VLA result\n            response = self.generate_conversational_response(\n                self.pending_user_query, msg.response, msg.success)\n\n            # Publish response\n            response_msg = String()\n            response_msg.data = response\n            self.robot_speech_pub.publish(response_msg)\n\n            # Add to conversation history\n            self.conversation_history.append({"role": "assistant", "content": response})\n\n            # Reset state\n            self.waiting_for_vla_response = False\n            self.pending_user_query = None\n\n    def process_user_input(self, user_input):\n        """Process user input and determine appropriate response"""\n        # Check if this is a command that needs VLA processing\n        if self.is_command_that_needs_vla(user_input):\n            # Forward to VLA system\n            self.vla_command_pub.publish(String(data=user_input))\n\n            # Set state to wait for VLA response\n            self.waiting_for_vla_response = True\n            self.pending_user_query = user_input\n\n            # Return acknowledgment\n            return "I\'m processing your request. Please wait a moment."\n        else:\n            # Handle as a conversational query\n            return self.handle_conversational_query(user_input)\n\n    def is_command_that_needs_vla(self, text):\n        """Determine if text is a command requiring VLA processing"""\n        action_keywords = [\n            \'go to\', \'navigate\', \'move to\', \'pick up\', \'grasp\', \'get\', \'take\',\n            \'put\', \'place\', \'clean\', \'wipe\', \'find\', \'locate\', \'bring\',\n            \'move the\', \'pick the\', \'go\', \'walk\', \'drive\', \'go there\'\n        ]\n\n        text_lower = text.lower()\n        return any(keyword in text_lower for keyword in action_keywords)\n\n    def handle_conversational_query(self, query):\n        """Handle conversational queries (not commands)"""\n        # Use LLM for conversational responses\n        messages = list(self.conversation_history)\n        messages.append({"role": "user", "content": query})\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=messages,\n                temperature=0.7,\n                max_tokens=150\n            )\n\n            return response.choices[0].message[\'content\']\n        except Exception as e:\n            self.get_logger().error(f\'LLM error: {e}\')\n            return "I\'m sorry, I didn\'t understand that. Could you please rephrase?"\n\n    def generate_conversational_response(self, original_query, vla_result, success):\n        """Generate a conversational response based on VLA result"""\n        # Use LLM to create a natural response\n        prompt = f"""\n        Original query: "{original_query}"\n        VLA system result: "{vla_result}"\n        Success: {success}\n\n        Generate a natural, conversational response that acknowledges the result.\n        Make it sound like a helpful robot assistant responding to a human.\n        Keep it concise but friendly.\n\n        Response:\n        """\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.7,\n                max_tokens=100\n            )\n\n            return response.choices[0].message[\'content\']\n        except Exception as e:\n            self.get_logger().error(f\'LLM error: {e}\')\n            return f"I\'ve completed the task. The result was: {vla_result}"\n\ndef main(args=None):\n    rclpy.init(args=args)\n    dialogue_manager = RobotDialogueManager()\n\n    try:\n        rclpy.spin(dialogue_manager)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        dialogue_manager.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"autonomous-humanoid-system",children:"Autonomous Humanoid System"}),"\n",(0,o.jsx)(n.h3,{id:"complete-autonomous-system-node",children:"Complete Autonomous System Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, JointState\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom std_msgs.msg import String, Bool\nfrom vision_language_interfaces.msg import VisionLanguageResponse\nfrom nav_msgs.msg import Odometry\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport asyncio\nimport threading\nfrom queue import Queue\nimport time\n\nclass AutonomousHumanoidSystem(Node):\n    def __init__(self):\n        super().__init__(\'autonomous_humanoid_system\')\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.command_queue = Queue()\n        self.emergency_stop = False\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n        self.odom_sub = self.create_subscription(\n            Odometry, \'/odom\', self.odom_callback, 10)\n        self.joint_state_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_state_callback, 10)\n        self.voice_command_sub = self.create_subscription(\n            String, \'/vla/voice_command\', self.voice_command_callback, 10)\n        self.emergency_stop_sub = self.create_subscription(\n            Bool, \'/emergency_stop\', self.emergency_stop_callback, 10)\n\n        # Create publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.joint_cmd_pub = self.create_publisher(JointState, \'/joint_commands\', 10)\n        self.speech_pub = self.create_publisher(String, \'/robot/speech_output\', 10)\n        self.status_pub = self.create_publisher(String, \'/system_status\', 10)\n\n        # Initialize state\n        self.current_image = None\n        self.current_pose = None\n        self.joint_states = {}\n        self.system_active = True\n\n        # Start autonomous processing thread\n        self.autonomous_thread = threading.Thread(target=self.autonomous_loop, daemon=True)\n        self.autonomous_thread.start()\n\n        self.get_logger().info(\'Autonomous Humanoid System initialized\')\n\n    def image_callback(self, msg):\n        """Handle camera images"""\n        try:\n            self.current_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n        except Exception as e:\n            self.get_logger().error(f\'Image processing error: {e}\')\n\n    def odom_callback(self, msg):\n        """Handle odometry data"""\n        self.current_pose = msg.pose.pose\n\n    def joint_state_callback(self, msg):\n        """Handle joint state data"""\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_states[name] = msg.position[i]\n\n    def voice_command_callback(self, msg):\n        """Handle voice commands"""\n        self.get_logger().info(f\'Received voice command: {msg.data}\')\n        self.command_queue.put(msg.data)\n\n    def emergency_stop_callback(self, msg):\n        """Handle emergency stop"""\n        self.emergency_stop = msg.data\n        if self.emergency_stop:\n            self.get_logger().warn(\'EMERGENCY STOP ACTIVATED\')\n            self.stop_robot()\n\n    def stop_robot(self):\n        """Stop all robot motion"""\n        # Stop base movement\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n\n        # Stop joint movements\n        joint_cmd = JointState()\n        joint_cmd.header.stamp = self.get_clock().now().to_msg()\n        self.joint_cmd_pub.publish(joint_cmd)\n\n    def autonomous_loop(self):\n        """Main autonomous processing loop"""\n        while rclpy.ok() and self.system_active:\n            try:\n                # Check for new commands\n                if not self.command_queue.empty():\n                    command = self.command_queue.get()\n                    self.process_command(command)\n\n                # Perform autonomous behaviors\n                self.perform_autonomous_behaviors()\n\n                # Check system status\n                self.check_system_status()\n\n                # Small delay to prevent excessive CPU usage\n                time.sleep(0.1)\n\n            except Exception as e:\n                self.get_logger().error(f\'Autonomous loop error: {e}\')\n                time.sleep(0.1)  # Brief pause before continuing\n\n    def process_command(self, command):\n        """Process a high-level command"""\n        if self.emergency_stop:\n            self.speak("System is in emergency stop mode. Please clear the emergency before proceeding.")\n            return\n\n        try:\n            # Parse command and execute appropriate behavior\n            if "clean" in command.lower():\n                self.execute_cleaning_task()\n            elif "navigate" in command.lower() or "go to" in command.lower():\n                self.execute_navigation_task(command)\n            elif "grasp" in command.lower() or "pick up" in command.lower():\n                self.execute_manipulation_task(command)\n            else:\n                self.speak(f"I received the command: {command}. I\'m working on it.")\n\n        except Exception as e:\n            self.get_logger().error(f\'Command execution error: {e}\')\n            self.speak("I encountered an error while executing the command.")\n\n    def execute_cleaning_task(self):\n        """Execute cleaning task"""\n        self.speak("Starting cleaning task.")\n\n        # Example cleaning sequence\n        cleaning_sequence = [\n            ("inspect_area", {}),\n            ("navigate_to_dirty_spot", {"target": "kitchen_counter"}),\n            ("clean_surface", {"surface": "counter", "method": "wipe"}),\n            ("inspect_result", {}),\n            ("report_completion", {})\n        ]\n\n        for action, params in cleaning_sequence:\n            if self.emergency_stop:\n                break\n            self.execute_single_action(action, params)\n\n        self.speak("Cleaning task completed.")\n\n    def execute_navigation_task(self, command):\n        """Execute navigation task"""\n        self.speak(f"Navigating as requested: {command}")\n\n        # In real implementation, this would parse the destination\n        # and execute navigation to that location\n        # For demonstration, we\'ll just move forward\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.2  # Move forward slowly\n        cmd_vel.angular.z = 0.0\n\n        # Publish for 2 seconds\n        for _ in range(20):  # 20 iterations * 0.1s = 2 seconds\n            if self.emergency_stop:\n                break\n            self.cmd_vel_pub.publish(cmd_vel)\n            time.sleep(0.1)\n\n        self.speak("Navigation task completed.")\n\n    def execute_manipulation_task(self, command):\n        """Execute manipulation task"""\n        self.speak(f"Performing manipulation: {command}")\n\n        # In real implementation, this would involve:\n        # 1. Object detection and localization\n        # 2. Motion planning for arms\n        # 3. Grasping execution\n        # 4. Verification\n        # For demonstration, we\'ll just report completion\n\n        self.speak("Manipulation task completed.")\n\n    def execute_single_action(self, action, params):\n        """Execute a single action with parameters"""\n        self.get_logger().info(f\'Executing action: {action} with params: {params}\')\n\n        # Map action to robot commands\n        if action == "navigate_to_dirty_spot":\n            # Navigate to a specific location\n            cmd_vel = Twist()\n            cmd_vel.linear.x = 0.1\n            self.cmd_vel_pub.publish(cmd_vel)\n            time.sleep(1.0)\n        elif action == "clean_surface":\n            # Simulate cleaning action\n            self.get_logger().info(f\'Cleaning {params.get("surface", "unknown")} surface\')\n            time.sleep(2.0)\n        elif action == "inspect_area":\n            # Use sensors to inspect area\n            if self.current_image is not None:\n                self.get_logger().info(\'Area inspection completed\')\n            time.sleep(0.5)\n\n    def perform_autonomous_behaviors(self):\n        """Perform background autonomous behaviors"""\n        # Safety monitoring\n        if self.current_pose:\n            # Check if robot is in safe position\n            pass\n\n        # Environmental monitoring\n        if self.current_image:\n            # Process image for obstacles, people, etc.\n            pass\n\n        # System health monitoring\n        if time.time() % 10 < 0.1:  # Every 10 seconds\n            self.publish_system_status()\n\n    def check_system_status(self):\n        """Check overall system status"""\n        # Check if all required nodes are running\n        # Check battery level (if available)\n        # Check for errors or warnings\n        pass\n\n    def publish_system_status(self):\n        """Publish system status"""\n        status_msg = String()\n        status_msg.data = f"Active - Image: {\'Yes\' if self.current_image is not None else \'No\'}, " \\\n                         f"Pose: {\'Yes\' if self.current_pose is not None else \'No\'}, " \\\n                         f"Joints: {len(self.joint_states)} available"\n        self.status_pub.publish(status_msg)\n\n    def speak(self, text):\n        """Publish speech output"""\n        speech_msg = String()\n        speech_msg.data = text\n        self.speech_pub.publish(speech_msg)\n        self.get_logger().info(f\'Robot says: {text}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    system = AutonomousHumanoidSystem()\n\n    try:\n        rclpy.spin(system)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        system.system_active = False\n        system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"launch-file-for-complete-vla-system",children:"Launch File for Complete VLA System"}),"\n",(0,o.jsx)(n.h3,{id:"complete-vla-system-launch",children:"Complete VLA System Launch"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<launch>\n  \x3c!-- Arguments --\x3e\n  <arg name="use_sim_time" default="false"/>\n  <arg name="robot_namespace" default="robot"/>\n  <arg name="camera_namespace" default="camera"/>\n\n  \x3c!-- VLA Command Processor --\x3e\n  <node pkg="vla_system" exec="vla_command_processor" name="vla_command_processor"\n        namespace="$(var robot_namespace)" output="screen">\n    <param name="use_sim_time" value="$(var use_sim_time)"/>\n    <remap from="/camera/image_raw" to="$(var camera_namespace)/image_raw"/>\n  </node>\n\n  \x3c!-- Robot Dialogue Manager --\x3e\n  <node pkg="vla_system" exec="robot_dialogue_manager" name="robot_dialogue_manager"\n        namespace="$(var robot_namespace)" output="screen">\n    <param name="use_sim_time" value="$(var use_sim_time)"/>\n  </node>\n\n  \x3c!-- Autonomous Humanoid System --\x3e\n  <node pkg="vla_system" exec="autonomous_humanoid_system" name="autonomous_humanoid_system"\n        namespace="$(var robot_namespace)" output="screen">\n    <param name="use_sim_time" value="$(var use_sim_time)"/>\n    <remap from="/camera/image_raw" to="$(var camera_namespace)/image_raw"/>\n    <remap from="/odom" to="odom"/>\n    <remap from="/joint_states" to="joint_states"/>\n  </node>\n\n  \x3c!-- Navigation Stack (if needed) --\x3e\n  <include file="$(find nav2_bringup)/launch/navigation_launch.py">\n    <arg name="use_sim_time" value="$(var use_sim_time)"/>\n  </include>\n\n  \x3c!-- Manipulation Stack (if needed) --\x3e\n  \x3c!-- Add manipulation stack launch if robot has arms --\x3e\n\n  \x3c!-- RViz for visualization --\x3e\n  <node pkg="rviz2" exec="rviz2" name="rviz" args="-d $(find vla_system)/config/vla_system.rviz"\n        output="screen"/>\n</launch>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"capstone-project-autonomous-humanoid-demonstration",children:"Capstone Project: Autonomous Humanoid Demonstration"}),"\n",(0,o.jsx)(n.h3,{id:"complete-system-integration-example",children:"Complete System Integration Example"}),"\n",(0,o.jsx)(n.p,{children:"The capstone project integrates all VLA components into a complete demonstration:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nComplete VLA System for Autonomous Humanoid Demonstration\nThis script coordinates all VLA components for the final project.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import Twist\nimport time\nimport threading\n\nclass VLACapstoneSystem(Node):\n    def __init__(self):\n        super().__init__(\'vla_capstone_system\')\n\n        # Create publishers\n        self.speech_pub = self.create_publisher(String, \'/robot/speech_output\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.emergency_stop_pub = self.create_publisher(Bool, \'/emergency_stop\', 10)\n\n        # System state\n        self.system_ready = False\n        self.demo_running = False\n\n        # Start initialization\n        self.initialize_system()\n\n    def initialize_system(self):\n        """Initialize the complete VLA system"""\n        self.speak("Initializing Vision-Language-Action system for autonomous humanoid demonstration.")\n\n        # In a real system, this would wait for all required nodes\n        time.sleep(2.0)  # Simulate initialization time\n\n        self.system_ready = True\n        self.speak("System initialization complete. Ready for demonstration commands.")\n\n    def run_demo_sequence(self):\n        """Run the complete demo sequence"""\n        if not self.system_ready:\n            self.speak("System not ready. Please wait for initialization.")\n            return\n\n        self.demo_running = True\n        self.speak("Starting autonomous humanoid demonstration.")\n\n        # Demo sequence\n        demo_steps = [\n            ("Introduce yourself", self.introduction),\n            ("Navigate to kitchen", self.navigate_to_kitchen),\n            ("Find and clean counter", self.clean_counter),\n            ("Return to starting position", self.return_home),\n            ("Demonstration complete", self.conclusion)\n        ]\n\n        for step_name, step_func in demo_steps:\n            if not self.demo_running:\n                break\n            self.speak(f"Executing: {step_name}")\n            step_func()\n            time.sleep(1.0)  # Brief pause between steps\n\n        self.speak("Autonomous humanoid demonstration completed successfully!")\n\n    def introduction(self):\n        """Introduce the system"""\n        self.speak("Hello! I am an autonomous humanoid robot powered by Vision-Language-Action technology. "\n                  "I can understand natural language commands, perceive my environment visually, "\n                  "and execute complex tasks in response.")\n\n    def navigate_to_kitchen(self):\n        """Navigate to kitchen area"""\n        self.speak("Navigating to the kitchen area.")\n        # Simulate navigation\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.2  # Move forward\n        for _ in range(10):  # Move for 1 second\n            self.cmd_vel_pub.publish(cmd_vel)\n            time.sleep(0.1)\n\n        # Stop\n        cmd_vel.linear.x = 0.0\n        self.cmd_vel_pub.publish(cmd_vel)\n\n    def clean_counter(self):\n        """Clean the counter"""\n        self.speak("Locating and cleaning the kitchen counter.")\n        # Simulate cleaning action\n        time.sleep(3.0)\n        self.speak("Counter cleaning completed. Surface is now clean.")\n\n    def return_home(self):\n        """Return to starting position"""\n        self.speak("Returning to starting position.")\n        # Simulate returning\n        cmd_vel = Twist()\n        cmd_vel.linear.x = -0.2  # Move backward\n        for _ in range(10):  # Move for 1 second\n            self.cmd_vel_pub.publish(cmd_vel)\n            time.sleep(0.1)\n\n        # Stop\n        cmd_vel.linear.x = 0.0\n        self.cmd_vel_pub.publish(cmd_vel)\n\n    def conclusion(self):\n        """Conclude the demonstration"""\n        self.speak("This concludes the Vision-Language-Action demonstration. "\n                  "I have successfully interpreted natural language commands, "\n                  "perceived my environment visually, and executed complex tasks autonomously. "\n                  "Thank you for watching!")\n\n    def speak(self, text):\n        """Publish speech output"""\n        speech_msg = String()\n        speech_msg.data = text\n        self.speech_pub.publish(speech_msg)\n        self.get_logger().info(f\'Robot says: {text}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    capstone_system = VLACapstoneSystem()\n\n    try:\n        # Run demo after a short delay to allow system to fully initialize\n        time.sleep(3.0)\n        capstone_system.run_demo_sequence()\n\n        # Keep node alive briefly to ensure all messages are sent\n        time.sleep(2.0)\n\n    except KeyboardInterrupt:\n        capstone_system.get_logger().info(\'Demo interrupted by user\')\n    finally:\n        capstone_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"practical-exercise-complete-vla-system",children:"Practical Exercise: Complete VLA System"}),"\n",(0,o.jsx)(n.p,{children:"Create a complete VLA demonstration system that:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integrates all components"}),": Vision, Language, and Action"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Processes natural language commands"}),': "Clean the counter", "Go to the kitchen"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Performs autonomous behaviors"}),": Navigation, manipulation, interaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Provides feedback"}),": Natural language responses to user"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Setup the complete system"})," with all required nodes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Create a launch file"})," that starts the entire VLA system"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Implement the capstone demonstration"})," that showcases all capabilities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Test with voice commands"})," to validate the complete pipeline"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Deploy to simulation"})," (or real robot if available) to demonstrate functionality"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting-vla-systems",children:"Troubleshooting VLA Systems"}),"\n",(0,o.jsx)(n.h3,{id:"1-integration-issues",children:"1. Integration Issues"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Components not communicating properly"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Check topic names, message types, and frame IDs"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"2-performance-issues",children:"2. Performance Issues"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Slow response times or dropped messages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Optimize data processing, adjust queue sizes, tune LLM calls"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"3-safety-concerns",children:"3. Safety Concerns"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Robot performing unsafe actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Implement safety checks, emergency stops, and validation layers"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"4-recognition-errors",children:"4. Recognition Errors"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Misunderstanding commands or misidentifying objects"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Improve training data, add validation steps, implement confirmation requests"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This lesson covered the complete integration of Vision-Language-Action systems for autonomous humanoid robots. We implemented components for voice processing, natural language understanding, task planning, action execution, and conversational interfaces. The capstone project demonstrates how all these components work together to create truly autonomous robots that can understand and respond to natural human commands."}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"This concludes the Physical AI & Humanoid Robotics textbook. You now have the knowledge to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Build and program robots using ROS 2"}),"\n",(0,o.jsx)(n.li,{children:"Create realistic simulations with Gazebo and Isaac Sim"}),"\n",(0,o.jsx)(n.li,{children:"Implement AI-powered perception and decision-making"}),"\n",(0,o.jsx)(n.li,{children:"Integrate vision, language, and action systems for autonomous operation"}),"\n",(0,o.jsx)(n.li,{children:"Deploy complete robotic systems that can interact naturally with humans"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>i});var t=s(6540);const o={},a=t.createContext(o);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);