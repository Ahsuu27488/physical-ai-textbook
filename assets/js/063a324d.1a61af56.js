"use strict";(globalThis.webpackChunkmy_website_1=globalThis.webpackChunkmy_website_1||[]).push([[9722],{3731:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4/lesson-1-vision-language-fundamentals","title":"Lesson 1 - Vision-Language Fundamentals","description":"Learning Objectives","source":"@site/docs/module-4/lesson-1-vision-language-fundamentals.md","sourceDirName":"module-4","slug":"/module-4/lesson-1-vision-language-fundamentals","permalink":"/physical-ai-textbook/docs/module-4/lesson-1-vision-language-fundamentals","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/lesson-1-vision-language-fundamentals.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Lesson 1 - Vision-Language Fundamentals","sidebar_position":4},"sidebar":"textbookSidebar","previous":{"title":"Module 4 - Vision-Language-Action (VLA)","permalink":"/physical-ai-textbook/docs/module-4/intro"},"next":{"title":"Lesson 2 - Cognitive Planning and Action Generation","permalink":"/physical-ai-textbook/docs/module-4/lesson-2-cognitive-planning"}}');var t=s(4848),o=s(8453);const a={title:"Lesson 1 - Vision-Language Fundamentals",sidebar_position:4},r="Lesson 1: Vision-Language Fundamentals",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Vision-Language Models",id:"introduction-to-vision-language-models",level:2},{value:"Key Concepts in Vision-Language AI",id:"key-concepts-in-vision-language-ai",level:3},{value:"Vision-Language Model Architectures",id:"vision-language-model-architectures",level:2},{value:"CLIP (Contrastive Language-Image Pretraining)",id:"clip-contrastive-language-image-pretraining",level:3},{value:"BLIP (Bootstrapping Language-Image Pre-training)",id:"blip-bootstrapping-language-image-pre-training",level:3},{value:"Visual Question Answering (VQA)",id:"visual-question-answering-vqa",level:2},{value:"Implementing Vision-Language Pipelines",id:"implementing-vision-language-pipelines",level:2},{value:"Basic Object Detection + Language Integration",id:"basic-object-detection--language-integration",level:3},{value:"Vision-Language Integration with ROS 2",id:"vision-language-integration-with-ros-2",level:2},{value:"Creating a Vision-Language ROS 2 Node",id:"creating-a-vision-language-ros-2-node",level:3},{value:"Cross-Modal Attention Mechanisms",id:"cross-modal-attention-mechanisms",level:2},{value:"Practical Exercise: Build a Simple VQA System",id:"practical-exercise-build-a-simple-vqa-system",level:2},{value:"Example Implementation:",id:"example-implementation",level:3},{value:"Evaluation Metrics for Vision-Language Models",id:"evaluation-metrics-for-vision-language-models",level:2},{value:"CIDEr (Consensus-based Image Description Evaluation)",id:"cider-consensus-based-image-description-evaluation",level:3},{value:"VQA Accuracy",id:"vqa-accuracy",level:3},{value:"F1-Score",id:"f1-score",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-1-vision-language-fundamentals",children:"Lesson 1: Vision-Language Fundamentals"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this lesson, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the fundamentals of vision-language models"}),"\n",(0,t.jsx)(n.li,{children:"Implement basic visual question answering systems"}),"\n",(0,t.jsx)(n.li,{children:"Connect computer vision outputs to language models"}),"\n",(0,t.jsx)(n.li,{children:"Create simple vision-language pipelines"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-vision-language-models",children:"Introduction to Vision-Language Models"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language (VL) models represent a significant advancement in AI, bridging the gap between visual perception and natural language understanding. These models enable robots to understand and respond to commands based on visual input, making human-robot interaction more intuitive."}),"\n",(0,t.jsx)(n.h3,{id:"key-concepts-in-vision-language-ai",children:"Key Concepts in Vision-Language AI"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Grounding"}),": Connecting visual elements to linguistic concepts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multimodal Fusion"}),": Combining visual and textual information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cross-Modal Attention"}),": Focusing on relevant visual regions based on language"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Embodied Language Understanding"}),": Connecting language to physical actions"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"vision-language-model-architectures",children:"Vision-Language Model Architectures"}),"\n",(0,t.jsx)(n.h3,{id:"clip-contrastive-language-image-pretraining",children:"CLIP (Contrastive Language-Image Pretraining)"}),"\n",(0,t.jsx)(n.p,{children:"CLIP represents one of the foundational architectures for vision-language understanding:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport clip\nfrom PIL import Image\n\n# Load the model\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nmodel, preprocess = clip.load("ViT-B/32", device=device)\n\n# Prepare images and text\nimage = preprocess(Image.open("image.png")).unsqueeze(0).to(device)\ntext = clip.tokenize(["a photo of a cat", "a photo of a dog"]).to(device)\n\n# Get predictions\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n\n    logits_per_image, logits_per_text = model(image, text)\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\nprint("Label probs:", probs)  # prints: [[0.9927937 0.0072063]]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"blip-bootstrapping-language-image-pre-training",children:"BLIP (Bootstrapping Language-Image Pre-training)"}),"\n",(0,t.jsx)(n.p,{children:"BLIP combines vision and language models for tasks like image captioning:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\n# Load processor and model\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")\n\n# Load and process image\nraw_image = Image.open("image.jpg").convert(\'RGB\')\ninputs = processor(raw_image, return_tensors="pt")\n\n# Generate caption\nout = model.generate(**inputs)\ncaption = processor.decode(out[0], skip_special_tokens=True)\nprint(f"Generated caption: {caption}")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"visual-question-answering-vqa",children:"Visual Question Answering (VQA)"}),"\n",(0,t.jsx)(n.p,{children:"Visual Question Answering combines computer vision with natural language processing:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nfrom transformers import ViltProcessor, ViltForQuestionAnswering\nfrom PIL import Image\n\n# Load processor and model\nprocessor = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")\nmodel = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")\n\n# Prepare inputs\ntext = "How many cats are there?"\nimage = Image.open("cats_image.jpg")\n\ninputs = processor(image, text, return_tensors="pt")\n\n# Forward pass\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\n    idx = logits.argmax(-1).item()\n\n# Get answer\nanswer = model.config.id2label[idx]\nprint(f"Answer: {answer}")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"implementing-vision-language-pipelines",children:"Implementing Vision-Language Pipelines"}),"\n",(0,t.jsx)(n.h3,{id:"basic-object-detection--language-integration",children:"Basic Object Detection + Language Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nimport openai\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\n\nclass VisionLanguagePipeline:\n    def __init__(self):\n        # Initialize models\n        self.clip_model, self.clip_processor = CLIPModel.from_pretrained(\n            "openai/clip-vit-base-patch32"\n        ), CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n\n        # Object detection model (using YOLO as example)\n        self.yolo_net = cv2.dnn.readNetFromDarknet("yolo_config.cfg", "yolo_weights.weights")\n        self.yolo_classes = self.load_classes("coco.names")\n\n    def detect_objects(self, image_path):\n        """Detect objects in an image using YOLO"""\n        image = cv2.imread(image_path)\n        height, width, channels = image.shape\n\n        # Create blob from image\n        blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n        self.yolo_net.setInput(blob)\n        outputs = self.yolo_net.forward(self.get_output_layers())\n\n        # Process outputs\n        boxes, confidences, class_ids = self.process_yolo_output(outputs, width, height)\n\n        # Draw bounding boxes\n        detections = []\n        for i in range(len(boxes)):\n            x, y, w, h = boxes[i]\n            class_name = self.yolo_classes[class_ids[i]]\n            confidence = confidences[i]\n\n            detections.append({\n                "class": class_name,\n                "confidence": confidence,\n                "bbox": [x, y, w, h],\n                "center": [x + w//2, y + h//2]\n            })\n\n        return detections, image\n\n    def describe_objects(self, image_path, detections):\n        """Generate descriptions for detected objects"""\n        image = self.clip_processor(images=cv2.imread(image_path), return_tensors="pt")\n\n        # Create text prompts for each detected object\n        text_prompts = [f"a photo of {det[\'class\']}" for det in detections]\n        inputs = self.clip_processor(text=text_prompts, images=cv2.imread(image_path),\n                                     return_tensors="pt", padding=True)\n\n        # Get similarity scores\n        outputs = self.clip_model(**inputs)\n        logits_per_image = outputs.logits_per_image\n        probs = logits_per_image.softmax(dim=-1)\n\n        # Associate probabilities with detections\n        for i, det in enumerate(detections):\n            det["similarity"] = probs[0][i].item()\n\n        return detections\n\n    def answer_questions(self, image_path, questions):\n        """Answer questions about the image"""\n        answers = []\n        image = cv2.imread(image_path)\n\n        for question in questions:\n            # This would typically use a VQA model\n            # For simplicity, we\'ll use a basic approach\n            detections = self.detect_objects(image_path)\n\n            # Generate answer based on detections\n            answer = self.generate_answer_from_detections(detections, question)\n            answers.append({\n                "question": question,\n                "answer": answer\n            })\n\n        return answers\n\n    def get_output_layers(self):\n        """Get output layer names"""\n        layer_names = self.yolo_net.getLayerNames()\n        output_layers = [layer_names[i[0] - 1] for i in self.yolo_net.getUnconnectedOutLayers()]\n        return output_layers\n\n    def process_yolo_output(self, outputs, width, height):\n        """Process YOLO outputs to get bounding boxes"""\n        # Implementation for processing YOLO outputs\n        # This is a simplified version\n        boxes = []\n        confidences = []\n        class_ids = []\n\n        for output in outputs:\n            for detection in output:\n                scores = detection[5:]\n                class_id = np.argmax(scores)\n                confidence = scores[class_id]\n\n                if confidence > 0.5:  # Confidence threshold\n                    center_x = int(detection[0] * width)\n                    center_y = int(detection[1] * height)\n                    w = int(detection[2] * width)\n                    h = int(detection[3] * height)\n\n                    x = int(center_x - w / 2)\n                    y = int(center_y - h / 2)\n\n                    boxes.append([x, y, w, h])\n                    confidences.append(float(confidence))\n                    class_ids.append(class_id)\n\n        # Apply non-maximum suppression\n        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n\n        filtered_boxes = [boxes[i] for i in indices]\n        filtered_confidences = [confidences[i] for i in indices]\n        filtered_class_ids = [class_ids[i] for i in indices]\n\n        return filtered_boxes, filtered_confidences, filtered_class_ids\n\n    def load_classes(self, file_path):\n        """Load class names from file"""\n        with open(file_path, "r") as f:\n            classes = [line.strip() for line in f.readlines()]\n        return classes\n\n    def generate_answer_from_detections(self, detections, question):\n        """Generate answer based on detections"""\n        # This is a simplified implementation\n        # In practice, you would use a specialized VQA model\n\n        if "how many" in question.lower():\n            unique_classes = set(det["class"] for det in detections)\n            counts = {cls: sum(1 for det in detections if det["class"] == cls)\n                     for cls in unique_classes}\n\n            if len(unique_classes) == 1:\n                return f"There are {counts[list(unique_classes)[0]]} {list(unique_classes)[0]}(s)"\n            else:\n                return f"Multiple objects detected: {\', \'.join([f\'{count} {cls}\' for cls, count in counts.items()])}"\n\n        elif "what color" in question.lower():\n            # Simplified color detection\n            return "Color detection requires additional processing"\n\n        else:\n            return f"Detected objects: {\', \'.join([det[\'class\'] for det in detections[:5]])}"  # Limit to 5\n'})}),"\n",(0,t.jsx)(n.h2,{id:"vision-language-integration-with-ros-2",children:"Vision-Language Integration with ROS 2"}),"\n",(0,t.jsx)(n.h3,{id:"creating-a-vision-language-ros-2-node",children:"Creating a Vision-Language ROS 2 Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom vision_language_interfaces.msg import VisionLanguageResponse\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass VisionLanguageNode(Node):\n    def __init__(self):\n        super().__init__('vision_language_node')\n\n        # Initialize the pipeline\n        self.vl_pipeline = VisionLanguagePipeline()\n        self.bridge = CvBridge()\n\n        # Create subscribers and publishers\n        self.image_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.question_sub = self.create_subscription(\n            String,\n            'vl_question',\n            self.question_callback,\n            10\n        )\n\n        self.response_pub = self.create_publisher(\n            VisionLanguageResponse,\n            'vl_response',\n            10\n        )\n\n        self.current_image = None\n        self.get_logger().info('Vision-Language node initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Handle incoming image messages\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            # Save the image temporarily for processing\n            cv2.imwrite('/tmp/current_image.jpg', cv_image)\n            self.current_image = '/tmp/current_image.jpg'\n            self.get_logger().info('Image received and saved')\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def question_callback(self, msg):\n        \"\"\"Handle incoming questions\"\"\"\n        if self.current_image is None:\n            self.get_logger().warn('No image available for processing')\n            return\n\n        try:\n            # Process the question with the current image\n            answers = self.vl_pipeline.answer_questions(\n                self.current_image,\n                [msg.data]\n            )\n\n            # Create response message\n            response_msg = VisionLanguageResponse()\n            response_msg.question = msg.data\n            response_msg.answer = answers[0]['answer']\n            response_msg.timestamp = self.get_clock().now().to_msg()\n\n            # Publish response\n            self.response_pub.publish(response_msg)\n            self.get_logger().info(f'Answered: {answers[0][\"answer\"]}')\n        except Exception as e:\n            self.get_logger().error(f'Error processing question: {e}')\n"})}),"\n",(0,t.jsx)(n.h2,{id:"cross-modal-attention-mechanisms",children:"Cross-Modal Attention Mechanisms"}),"\n",(0,t.jsx)(n.p,{children:"Cross-modal attention allows the model to focus on relevant parts of one modality based on information from another:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\n\nclass CrossModalAttention(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.query_projection = nn.Linear(dim, dim)\n        self.key_projection = nn.Linear(dim, dim)\n        self.value_projection = nn.Linear(dim, dim)\n        self.scale = dim ** -0.5\n\n    def forward(self, visual_features, language_features):\n        """\n        Apply cross-modal attention\n        visual_features: [batch_size, num_visual_tokens, dim]\n        language_features: [batch_size, num_language_tokens, dim]\n        """\n        # Project features\n        Q = self.query_projection(visual_features)\n        K = self.key_projection(language_features)\n        V = self.value_projection(language_features)\n\n        # Calculate attention scores\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n\n        # Apply attention to values\n        attended_features = torch.matmul(attention_weights, V)\n\n        return attended_features\n\n# Usage in a multimodal model\nclass MultimodalFusion(nn.Module):\n    def __init__(self, dim=512):\n        super().__init__()\n        self.visual_encoder = nn.Linear(2048, dim)  # For ResNet features\n        self.text_encoder = nn.Linear(768, dim)     # For BERT features\n        self.cross_attention = CrossModalAttention(dim)\n        self.classifier = nn.Linear(dim, num_classes)\n\n    def forward(self, visual_input, text_input):\n        # Encode modalities\n        visual_features = self.visual_encoder(visual_input)\n        text_features = self.text_encoder(text_input)\n\n        # Apply cross-modal attention\n        attended_visual = self.cross_attention(visual_features, text_features)\n\n        # Combine features (simple concatenation or more complex fusion)\n        combined_features = attended_visual.mean(dim=1)  # Pool visual tokens\n\n        # Classify\n        output = self.classifier(combined_features)\n        return output\n'})}),"\n",(0,t.jsx)(n.h2,{id:"practical-exercise-build-a-simple-vqa-system",children:"Practical Exercise: Build a Simple VQA System"}),"\n",(0,t.jsx)(n.p,{children:"Create a basic visual question answering system that can answer simple questions about objects in an image:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Setup"}),": Install required packages (transformers, torch, opencv)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Detection"}),": Implement basic object detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Question Processing"}),": Parse simple questions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Answer Generation"}),": Generate answers based on detected objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration"}),": Create a ROS 2 node that processes camera images and questions"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example-implementation",children:"Example Implementation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def simple_vqa_system(image_path, question):\n    """\n    Simple VQA system that answers basic questions about objects in an image\n    """\n    # Load image\n    image = cv2.imread(image_path)\n\n    # Detect objects (using a pre-trained model)\n    detections = detect_objects_yolo(image)\n\n    # Parse question\n    question_lower = question.lower()\n\n    if "how many" in question_lower:\n        # Count objects\n        object_counts = {}\n        for det in detections:\n            obj_class = det[\'class\']\n            object_counts[obj_class] = object_counts.get(obj_class, 0) + 1\n\n        if len(object_counts) == 1:\n            obj_type = list(object_counts.keys())[0]\n            count = object_counts[obj_type]\n            return f"There are {count} {obj_type}(s)"\n        else:\n            result = []\n            for obj_type, count in object_counts.items():\n                result.append(f"{count} {obj_type}")\n            return f"I see: {\', \'.join(result)}"\n\n    elif "where is" in question_lower or "location" in question_lower:\n        # Find specific object\n        target_object = extract_object_from_question(question)\n        locations = []\n\n        for det in detections:\n            if target_object in det[\'class\'] or det[\'class\'] in target_object:\n                center_x, center_y = det[\'center\']\n                # Convert to relative position\n                h, w = image.shape[:2]\n                if center_y < h/3:\n                    vertical_pos = "top"\n                elif center_y < 2*h/3:\n                    vertical_pos = "middle"\n                else:\n                    vertical_pos = "bottom"\n\n                if center_x < w/3:\n                    horizontal_pos = "left"\n                elif center_x < 2*w/3:\n                    horizontal_pos = "center"\n                else:\n                    horizontal_pos = "right"\n\n                locations.append(f"{vertical_pos} {horizontal_pos}")\n\n        if locations:\n            return f"The {target_object} is in the {locations[0]} of the image"\n        else:\n            return f"I don\'t see any {target_object} in the image"\n\n    else:\n        # General description\n        unique_objects = list(set([det[\'class\'] for det in detections]))\n        return f"I see: {\', \'.join(unique_objects[:5])}"  # Limit to 5 objects\n\ndef extract_object_from_question(question):\n    """Extract object name from question"""\n    # Simple extraction - in practice, use NLP techniques\n    import re\n    # Look for common object words in the question\n    words = question.lower().split()\n    common_objects = ["person", "bicycle", "car", "motorbike", "aeroplane", "bus",\n                      "train", "truck", "boat", "traffic light", "fire hydrant",\n                      "stop sign", "parking meter", "bench", "bird", "cat", "dog",\n                      "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe",\n                      "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee",\n                      "skis", "snowboard", "sports ball", "kite", "baseball bat",\n                      "baseball glove", "skateboard", "surfboard", "tennis racket",\n                      "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl",\n                      "banana", "apple", "sandwich", "orange", "broccoli", "carrot",\n                      "hot dog", "pizza", "donut", "cake", "chair", "sofa", "pottedplant",\n                      "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse",\n                      "remote", "keyboard", "cell phone", "microwave", "oven", "toaster",\n                      "sink", "refrigerator", "book", "clock", "vase", "scissors",\n                      "teddy bear", "hair drier", "toothbrush"]\n\n    for word in words:\n        # Remove common question words\n        if word in [\'is\', \'are\', \'the\', \'a\', \'an\', \'where\', \'how\', \'many\', \'what\', \'on\', \'in\', \'at\']:\n            continue\n        # Check if word matches any common object\n        for obj in common_objects:\n            if word in obj or obj in word:\n                return obj\n\n    return "object"  # Default if no specific object found\n'})}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-metrics-for-vision-language-models",children:"Evaluation Metrics for Vision-Language Models"}),"\n",(0,t.jsx)(n.h3,{id:"cider-consensus-based-image-description-evaluation",children:"CIDEr (Consensus-based Image Description Evaluation)"}),"\n",(0,t.jsx)(n.p,{children:"For image captioning tasks"}),"\n",(0,t.jsx)(n.h3,{id:"vqa-accuracy",children:"VQA Accuracy"}),"\n",(0,t.jsx)(n.p,{children:"For visual question answering tasks"}),"\n",(0,t.jsx)(n.h3,{id:"f1-score",children:"F1-Score"}),"\n",(0,t.jsx)(n.p,{children:"For object detection and grounding tasks"}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This lesson introduced the fundamentals of vision-language models and how they can be implemented for robotics applications. Vision-language integration is essential for creating robots that can understand and respond to natural language commands based on visual input."}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"In the next lesson, we'll explore how to connect vision-language systems with robotic action planning and execution."})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>r});var i=s(6540);const t={},o=i.createContext(t);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);